---
title: "Class08: Breast Cancer Mini Project"
author: "Kalisa Kang (A16741690)"
format: pdf
---

## About

In today's lab, we will work with fine need aspiration (FNA) of breast mass data from the University of Wisconsin. 

## Data Import

```{r}
wisc.df <- read.csv("WisconsinCancer (1).csv", row.names = 1)
head(wisc.df)
```

> Q. How many patients/individuals/samples are in this dataset?

```{r}
nrow(wisc.df)
```

> Q. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

> Q. How many variables/features in the data are suffixed with _mean?

```{r}
ncol(wisc.df)
colnames(wisc.df)
```

```{r}
inds <- grep("_mean", colnames(wisc.df))

# This tells us where in the column "_mean" suffices are located (position 2, 3, 4, etc...)

length(inds)
```

```{r}
grep("_mean", colnames(wisc.df), value=T)
```

## Initial Analysis

Before analysis, I want to remove the expert diagnoses column (aka the answer) from our dataset. 

```{r}
# This stores the diagonsis column as a factor. See bottom of printout for Levels: B M
diagnosis <- as.factor(wisc.df$diagnosis)
head(diagnosis)
```

```{r}
#We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
```


## Clustering

We can try a kmeans() clustering first. 

```{r}
km <- kmeans(wisc.data, centers=2)
km$cluster
```

```{r}
table(km$cluster)
```

Cross-table

```{r}
table(km$cluster, diagnosis)
# Shows that cluster 1 has 1 benign and 130 malignant. Cluster 2 has 356 benign and 82 malignant.
```
Let's try `hclust()`. The key input required for `hclust()` is a distance matrix as produced by the `dist()` function.

```{r}
hc <- hclust(dist(wisc.data))
plot(hc)
# We can't really tell which groups are malignant and benign. We can cut it at the top, but it's still not clear, so we will do PCA.
```


## PCA

Do we need to scale the data?

We can look at the sd of each column (original variable).

```{r}
round(apply(wisc.data, 2, sd),)
```

Yes, we need to scale. We will run `prcomp()` with `scale=TRUE`.

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

Generate our main PCA plot (score plot, PC1 vs PC2 plot)...

```{r}
library(ggplot2)

res <- as.data.frame(wisc.pr$x)
```

```{r}
ggplot(res) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```
We can clearly see 2 clusters and can probably even draw a line to separate B and M. 

## Combining Methods

Clustering on PCA results

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage `method="ward.D2"`. We use Wardâ€™s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to `wisc.pr.hclust`.


```{r}
d <- dist(wisc.pr$x[,1:3])
hc <- hclust(d, method="ward.D2")
plot(hc)
```

To get my clustering result/membership vector, I need to "cut" the tree with the "cutree()`' function.

```{r}
grps <- cutree(hc, h=80)
```

> Q. How many patients are in each cluster group?

```{r}
table(cutree(hc, h=80))
```

```{r}
table(grps)
```

```{r}
plot(res$PC1, res$PC2, col=grps)
```

## Prediction

We can use our PCA result (model) to do predictions, i.e., taking new unseen data and projecting it onto our new PC variables.

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(res$PC1, res$PC2, col=grps)

# This plots the 2 patients on the plot.
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)

#This labels the 2 points.
text(npc[,1], npc[,2], labels=c(1,2), col="white")
```

> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

The hierarchical clustering of PCA resulted in the best sensitivity and specificity. The model with the best sensitivity should have the highest TP/(TP+FN).  The model with the best sensitivity should have the highest TN/(TN+FN).

> Q18. Which of these new patients should we prioritize for follow up based on your results?

We should prioritize a follow up for patient 2 because their diagnosis was malignant.

# Summary

Principal Component Analysis (PCA) is a super useful method for analyzing large datasets. It works by finding new variables (PCs) that capture the most variance from the original variables in your dataset. 










